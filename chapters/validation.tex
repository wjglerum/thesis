\chapter{Validation}
\label{chap:validation}
This chapter reports on how the \gls{poc} from Chapter~\ref{chap:proof-of-concept} answers \textbf{RQ2} and how it addresses the issues from \textbf{RQ1} as described in Chapter~\ref{chap:research}. Next we explore how this \gls{poc} manifests itself at Lunatech's automotive client.

\section{PoC}
The introduced \gls{poc} only addresses a part of the features from Table~\ref{tab:problems}. We only focused on network policies and not on creating and listing virtual networks. For the features relevant to network policies we explain how the \gls{poc} achieves them below.
\begin{itemize}
    \item[\textbf{List network policies}] Listing network policies was possible using the plugin specific \gls{api}. We generalised this \gls{api} and made it available for an operator, allowing her to see all the applied network policies in the cluster without diving into the concrete plugins for their specific \glspl{api}. In Listing~\ref{lst:policy} we see an example of the applied network policies, in this case we block all incoming traffic to containers with the role webserver on port 80.
    \item[\textbf{Create network policies}] Creating network policies leverages the same plugin model as listing the network policies. An operator is now able to create network policies from a central place.
\end{itemize}
\lstinputlisting[label={lst:policy},caption={Example of applied network policies}]{code/policy.json}

Listing the virtual networks created by \gls{cni} plugins is possible with the custom controller, as the state of the network is simply stored using the kube-apiserver. However creating the virtual networks deemed more difficult, requiring custom solutions to distribute the different plugin files and configuration files to every node. We do list how these features could impact the management of virtual networks for operators and developers.
\begin{itemize}
    \item[\textbf{List virtual networks}] When a network is created, the state gets stored in the custom controller. We can then can run a simply query to obtain the available networks in the cluster. This allows operators to get an overview of the networks, useful for debugging network problems or analysing network flows for example.
    \item[\textbf{Select virtual network}] When we have the list of available networks, developers can use this select the right network for their application. Instead of guessing the right network name or diving into the agent to see the configured networks.
    \item[\textbf{Create virtual network}] When there is a system in place to manage files across all agents, we could extend our controller to support the creation of virtual networks. An operator would provide the desired configuration and relevant files and the controller takes care of configuring the new virtual network on every agent. 
\end{itemize}

\section{Concrete use case}
An automotive client processes car data in a data processing pipeline for analytics. In this project there are  privacy concerns and the project should be usable by different tenants which calls for isolation of services and data flows. To achieve this isolation they use Project Calico on their \gls{dcos} cluster and enforce network policies. In the future they might use for example Cilium to protect certain Kafka topics or REST endpoints. At the moment it is hard for a developer to see which network policies are blocking the traffic, often leading to unwanted solutions where they simply try to remove their application from the virtual network. There is nothing visible in the \gls{dcos} web interface to inspect the created virtual networks and the applied network policies.

With the \gls{poc} in place we are able to transparently expose the network policies. At the moment this is not yet visible in the web interface of \gls{dcos}, however we could easily add this in a future version using the custom controller we have built. This will allow developers to inspect the relevant network policies for their applications and adjust them if needed. Furthermore our custom controller is plugin agnostic, meaning that a developer can inspect and create network policies for different plugins with the same interface. This follows the model from Kubernetes where developers can specify the wanted state and the controller uses the plugins to configure the relevant networks and policies. 

Using the custom controller we can also show to stakeholders, such as the central IT department, that indeed services are isolated from each other. This is especially important as the project has to be approved before it can be used with real data. With the new custom controller in place we can easily show the applied policies and adjust quickly if needed.
